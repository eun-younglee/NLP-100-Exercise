{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-jenny",
   "metadata": {},
   "source": [
    "# NLP 100 Exercise\n",
    "## Chapter 1: Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-horizontal",
   "metadata": {},
   "source": [
    "### 00. Reversed string\n",
    "Obtain the string that arranges letters of the string “stressed” in reverse order (tail to head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "natural-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "string = \"stressed\"\n",
    "print(string[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-number",
   "metadata": {},
   "source": [
    "### 01. “schooled”\n",
    "Obtain the string that concatenates the 1st, 3rd, 5th, and 7th letters in the string “schooled”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col\n"
     ]
    }
   ],
   "source": [
    "string = \"schooled\"\n",
    "print(string[1] + string[3] + string[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-heath",
   "metadata": {},
   "source": [
    "### 02. “shoe” + “cold” = “schooled”\n",
    "Obtain the string “schooled” by concatenating the letters in “shoe” and “cold” one after the other from head to tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fancy-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schooled\n"
     ]
    }
   ],
   "source": [
    "shoe = \"shoe\"\n",
    "cold = \"cold\"\n",
    "string = \"\"\n",
    "for i in range(len(shoe)): # add to string one by one\n",
    "    string += shoe[i]\n",
    "    string += cold[i]\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-courage",
   "metadata": {},
   "source": [
    "### 03. Pi\n",
    "Split the sentence “Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics”. into words, and create a list whose element presents the number of alphabetical letters in the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "creative-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "words = sentence.split()\n",
    "alphabets = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    alpha_count = 0\n",
    "    for j in range(len(words[i])):\n",
    "        if words[i][j].isalpha(): # isalpha() returns True if the character is an alphabet \n",
    "            alpha_count += 1\n",
    "    alphabets.append(alpha_count)\n",
    "        \n",
    "print(alphabets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-monday",
   "metadata": {},
   "source": [
    "### 04. Atomic symbols\n",
    "Split the sentence “Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can”. into words, and extract the first letter from the 1st, 5th, 6th, 7th, 8th, 9th, 15th, 16th, 19th words and the first two letters from the other words. Create an associative array (dictionary object or mapping object) that maps from the extracted string to the position (offset in the sentence) of the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "altered-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'H', 1: 'He', 2: 'Li', 3: 'Be', 4: 'B', 5: 'C', 6: 'N', 7: 'O', 8: 'F', 9: 'Ne', 10: 'Na', 11: 'Mi', 12: 'Al', 13: 'Si', 14: 'P', 15: 'S', 16: 'Cl', 17: 'Ar', 18: 'K', 19: 'Ca'}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can\"\n",
    "order = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "periodic_table = dict()\n",
    "words = sentence.split()\n",
    "\n",
    "for i in range(len(words)):\n",
    "    if (i + 1) in order: # if word is in order\n",
    "        periodic_table[i] = words[i][0]\n",
    "    else:\n",
    "        periodic[i] = words[i][:2]\n",
    "\n",
    "print(periodic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-thong",
   "metadata": {},
   "source": [
    "### 05. n-gram\n",
    "Implement a function that obtains n-grams from a given sequence object (e.g., string and list). Use this function to obtain word bi-grams and letter bi-grams from the sentence “I am an NLPer”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-polyester",
   "metadata": {},
   "source": [
    "make one function for both word bi-grams and letter bi-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tamil-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "['Ia', 'am', 'ma', 'an', 'nN', 'NL', 'LP', 'Pe', 'er']\n"
     ]
    }
   ],
   "source": [
    "def n_gram(string):\n",
    "    ngram = []\n",
    "    for i in range(len(string) - 1):\n",
    "        ngram.append(string[i:i + 2])\n",
    "    return ngram\n",
    "\n",
    "sentence = \"I am an NLPer\"\n",
    "word_bi_gram = n_gram(sentence.split()) # split by words\n",
    "letter_bi_gram = n_gram(\"\".join(sentence.split())) # get rid of spaces\n",
    "print(word_bi_gram)\n",
    "print(letter_bi_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-insight",
   "metadata": {},
   "source": [
    "### 06. Set\n",
    "Let the sets of letter bi-grams from the words “paraparaparadise” and “paragraph” $X$ and $Y$, respectively. Obtain the union, intersection, difference of the two sets. In addition, check whether the bigram “se” is included in the sets $X$ and $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expressed-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union: {'ap', 'se', 'is', 'ad', 'ph', 'ar', 'ra', 'di', 'ag', 'gr', 'pa'}\n",
      "Intersection: {'ap', 'ar', 'ra', 'pa'}\n",
      "Difference: {'is', 'ad', 'di', 'se'}\n",
      "\"se\" included in X: True\n",
      "\"se\" included in Y: False\n"
     ]
    }
   ],
   "source": [
    "def n_gram(string):\n",
    "    ngram = set() # has to be set to use union/intersection/difference\n",
    "    for i in range(len(string) - 1):\n",
    "        ngram.add(string[i:i + 2])\n",
    "    return ngram\n",
    "\n",
    "word1 = \"paraparaparadise\"\n",
    "word2 = \"paragraph\"\n",
    "X, Y = n_gram(word1), n_gram(word2)\n",
    "print(\"Union: {}\\nIntersection: {}\\nDifference: {}\".format(X.union(Y), X.intersection(Y), X.difference(Y)))\n",
    "print('\"se\" included in X: {}\\n\"se\" included in Y: {}'.format(True if \"se\" in X else False, True if \"se\" in Y else False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-timer",
   "metadata": {},
   "source": [
    "### 07. Template-based sentence generation\n",
    "Implement a function that receives arguments, x, y, and z and returns a string “{y} is {z} at {x}”, where “{x}”, “{y}”, and “{z}” denote the values of x, y, and z, respectively. In addition, confirm the return string by giving the arguments x=12, y=\"temperature\", and z=22.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ruled-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature is 22.4 at 12\n"
     ]
    }
   ],
   "source": [
    "def sentence(x, y, z):\n",
    "    return f\"{y} is {z} at {x}\"\n",
    "\n",
    "x, y, z = 12, \"temperature\", 22.4\n",
    "print(sentence(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-canon",
   "metadata": {},
   "source": [
    "### 08. cipher\n",
    "Implement a function cipher that converts a given string with the specification:\n",
    "\n",
    "Every alphabetical lowercase letter c is converted to a letter whose ASCII code is (219 - [the ASCII code of c])\n",
    "\n",
    "Keep other letters unchanged\n",
    "\n",
    "Use this function to cipher and decipher an English message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "defined-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "ciphered: Hvool Wliow!\n",
      "deciphered: Hello World!\n"
     ]
    }
   ],
   "source": [
    "def cipher(message):\n",
    "    ciphered = \"\"\n",
    "    for c in message:\n",
    "        if c.islower(): # if character is lowercase\n",
    "            ciphered += chr(219 - ord(c)) # convert to (26 - cth)th letter \n",
    "        else:\n",
    "            ciphered += c\n",
    "    return ciphered\n",
    "\n",
    "message = input()\n",
    "print(\"ciphered:\", result := cipher(message))\n",
    "print(\"deciphered:\", cipher(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-powder",
   "metadata": {},
   "source": [
    "### 09. Typoglycemia\n",
    "Write a program with the specification:\n",
    "\n",
    "Receive a word sequence separated by space\n",
    "\n",
    "For each word in the sequence:\n",
    "\n",
    "If the word is no longer than four letters, keep the word unchanged\n",
    "\n",
    "Otherwise, Keep the first and last letters unchanged\n",
    "\n",
    "Shuffle other letters in other positions (in the middle of the word)\n",
    "\n",
    "Observe the result by giving a sentence, e.g., \"I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "satisfactory-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cdlu’ont beevile that I cluod altualcy usetnnadrd what I was rdineag : the pmeahoennl power of the hamun mind\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def typo(sentence):\n",
    "    sentence = sentence.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if len(sentence[i]) > 4: # if word is longer than 4\n",
    "            characters = list(sentence[i].strip(\" \")) # dissect word into characters\n",
    "            to_shuffle = characters[1:-1] # leave first and last\n",
    "            random.shuffle(to_shuffle) # shuffle \n",
    "            sentence[i] = characters[0] + \"\".join(to_shuffle) + characters[-1] # put back again\n",
    "    return sentence\n",
    "            \n",
    "sentence = \"I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind \"\n",
    "sentence = typo(sentence)\n",
    "print(\" \".join(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
